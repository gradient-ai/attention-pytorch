{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f05f98-0163-4e28-9df6-13a287970540",
   "metadata": {},
   "source": [
    "# Set up \n",
    "We are using a dataset from Kaggle. All images are of skin lesions and of the shape 512 x 512 x 3. To set up the Kaggle API and download the dataset into a Gradient Notebook to download this data, follow these steps:\n",
    "\n",
    "First, create and log in to a Kaggle account\n",
    "\n",
    "Second, create an API token by going to your Account settings, and save kaggle.json on to your local machine\n",
    "\n",
    "Third, Upload kaggle.json to the Gradient NotebookFourth, move the file to ~/.kaggle/ using the terminal command `mv kaggle.json ~/.kaggle/`\n",
    "\n",
    "Fourth, use the API to download the dataset via the terminal: `kaggle datasets download shonenkov/melanoma-merged-external-data-512x512-jpeg`\n",
    "\n",
    "Fifth, use the terminal to unzip the dataset: unzip melanoma-merged-external-data-512x512-jpeg.zip\n",
    "\n",
    "link: https://www.kaggle.com/datasets/shonenkov/melanoma-merged-external-data-512x512-jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3da8581-0ea8-4ad5-882e-e950444e4e5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T21:15:59.906199Z",
     "iopub.status.busy": "2022-06-15T21:15:59.905607Z",
     "iopub.status.idle": "2022-06-15T21:16:03.797541Z",
     "shell.execute_reply": "2022-06-15T21:16:03.796804Z",
     "shell.execute_reply.started": "2022-06-15T21:15:59.906120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 16.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10 in /opt/conda/lib/python3.8/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from kaggle) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.8/site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from kaggle) (2.26.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from kaggle) (4.62.3)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/lib/python3.8/site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.8/site-packages (from kaggle) (1.26.7)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.8/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->kaggle) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->kaggle) (3.1)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73052 sha256=a07317c050e0c8bac3fa30330dfd00effb03bd331ce10cfaad56456ef3fe1de7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-efqqpzxw/wheels/29/da/11/144cc25aebdaeb4931b231e25fd34b394e6a5725cbb2f50106\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle\n",
      "Successfully installed kaggle-1.5.12\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d5e1100-5d58-4be9-b493-170980077e27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T22:04:25.087990Z",
     "iopub.status.busy": "2022-06-15T22:04:25.087704Z",
     "iopub.status.idle": "2022-06-15T22:04:27.022782Z",
     "shell.execute_reply": "2022-06-15T22:04:27.022096Z",
     "shell.execute_reply.started": "2022-06-15T22:04:25.087917Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc367c8d-105e-4878-83e6-c118008ab852",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T22:04:27.024360Z",
     "iopub.status.busy": "2022-06-15T22:04:27.024174Z",
     "iopub.status.idle": "2022-06-15T22:04:27.349235Z",
     "shell.execute_reply": "2022-06-15T22:04:27.348627Z",
     "shell.execute_reply.started": "2022-06-15T22:04:27.024339Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the data \n",
    "data_dir='512x512-dataset-melanoma/512x512-dataset-melanoma/'\n",
    "data=pd.read_csv('marking.csv')\n",
    "\n",
    "# balance the data a bit\n",
    "df_0=data[data['target']==0].sample(6000,random_state=42)\n",
    "df_1=data[data['target']==1]\n",
    "data=pd.concat([df_0,df_1]).reset_index()\n",
    "\n",
    "#prepare the data\n",
    "labels=[]\n",
    "images=[]\n",
    "for i in range(data.shape[0]):\n",
    "    images.append(data_dir + data['image_id'].iloc[i]+'.jpg')\n",
    "    labels.append(data['target'].iloc[i])\n",
    "df=pd.DataFrame(images)\n",
    "df.columns=['images']\n",
    "df['target']=labels\n",
    "\n",
    "# Split train into train and val\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['images'],df['target'], test_size=0.2, random_state=1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2acbb5b-8640-43ff-9133-039bb85754bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T22:04:27.350267Z",
     "iopub.status.busy": "2022-06-15T22:04:27.350097Z",
     "iopub.status.idle": "2022-06-15T22:04:27.355583Z",
     "shell.execute_reply": "2022-06-15T22:04:27.355072Z",
     "shell.execute_reply.started": "2022-06-15T22:04:27.350245Z"
    }
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "        transforms.RandomRotation(10),      # rotate +/- 10 degrees\n",
    "        transforms.RandomHorizontalFlip(),  # reverse 50% of images\n",
    "        transforms.Resize(224),             # resize shortest side to 224 pixels\n",
    "        transforms.CenterCrop(224),         # crop longest side to 224 pixels at center\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a67fdd7-1dc8-4b2b-8629-71f104d2a2df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T22:05:04.626524Z",
     "iopub.status.busy": "2022-06-15T22:05:04.625855Z",
     "iopub.status.idle": "2022-06-15T22:05:04.633534Z",
     "shell.execute_reply": "2022-06-15T22:05:04.632929Z",
     "shell.execute_reply.started": "2022-06-15T22:05:04.626492Z"
    }
   },
   "outputs": [],
   "source": [
    "from cv2 import cv2 \n",
    "from PIL import Image\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,data_paths,labels,transform=None,mode='train'):\n",
    "         self.data=data_paths\n",
    "         self.labels=labels\n",
    "         self.transform=transform\n",
    "         self.mode=mode\n",
    "    def __len__(self):\n",
    "       return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img_name = self.data[idx]\n",
    "        img = cv2.imread(img_name)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img=Image.fromarray(img)\n",
    "        if self.transform is not None:\n",
    "          img = self.transform(img)\n",
    "        img=img.cuda()\n",
    "        \n",
    "        labels = torch.tensor(self.labels[idx]).cuda()\n",
    "\n",
    "        return img, labels\n",
    "            \n",
    "train_dataset=ImageDataset(data_paths=X_train.values,labels=y_train.values,transform=train_transform)\n",
    "val_dataset=ImageDataset(data_paths=X_val.values,labels=y_val.values,transform=val_transform)\n",
    "\n",
    "train_loader=DataLoader(train_dataset,batch_size=100,shuffle=True)\n",
    "val_loader=DataLoader(val_dataset,batch_size=50,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecb41774-c1ed-4ba6-addd-b77ec96e0183",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T22:05:05.177855Z",
     "iopub.status.busy": "2022-06-15T22:05:05.177665Z",
     "iopub.status.idle": "2022-06-15T22:05:05.186123Z",
     "shell.execute_reply": "2022-06-15T22:05:05.185577Z",
     "shell.execute_reply.started": "2022-06-15T22:05:05.177839Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_features_l, in_features_g, attn_features, up_factor, normalize_attn=True):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.up_factor = up_factor\n",
    "        self.normalize_attn = normalize_attn\n",
    "        self.W_l = nn.Conv2d(in_channels=in_features_l, out_channels=attn_features, kernel_size=1, padding=0, bias=False)\n",
    "        self.W_g = nn.Conv2d(in_channels=in_features_g, out_channels=attn_features, kernel_size=1, padding=0, bias=False)\n",
    "        self.phi = nn.Conv2d(in_channels=attn_features, out_channels=1, kernel_size=1, padding=0, bias=True)\n",
    "    def forward(self, l, g):\n",
    "        N, C, W, H = l.size()\n",
    "        l_ = self.W_l(l)\n",
    "        g_ = self.W_g(g)\n",
    "        if self.up_factor > 1:\n",
    "            g_ = F.interpolate(g_, scale_factor=self.up_factor, mode='bilinear', align_corners=False)\n",
    "        c = self.phi(F.relu(l_ + g_)) # batch_sizex1xWxH\n",
    "        \n",
    "        # compute attn map\n",
    "        if self.normalize_attn:\n",
    "            a = F.softmax(c.view(N,1,-1), dim=2).view(N,1,W,H)\n",
    "        else:\n",
    "            a = torch.sigmoid(c)\n",
    "        # re-weight the local feature\n",
    "        f = torch.mul(a.expand_as(l), l) # batch_sizexCxWxH\n",
    "        if self.normalize_attn:\n",
    "            output = f.view(N,C,-1).sum(dim=2) # weighted sum\n",
    "        else:\n",
    "            output = F.adaptive_avg_pool2d(f, (1,1)).view(N,C) # global average pooling\n",
    "        return a, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "899b1910-5f4c-4184-b044-aaf5213ac90a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T22:07:08.302406Z",
     "iopub.status.busy": "2022-06-15T22:07:08.301529Z",
     "iopub.status.idle": "2022-06-15T22:07:08.316263Z",
     "shell.execute_reply": "2022-06-15T22:07:08.315722Z",
     "shell.execute_reply.started": "2022-06-15T22:07:08.302371Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class AttnVGG(nn.Module):\n",
    "    def __init__(self, num_classes, normalize_attn=False, dropout=None):\n",
    "        super(AttnVGG, self).__init__()\n",
    "        net = models.vgg16_bn(pretrained=True)\n",
    "        self.conv_block1 = nn.Sequential(*list(net.features.children())[0:6])\n",
    "        self.conv_block2 = nn.Sequential(*list(net.features.children())[7:13])\n",
    "        self.conv_block3 = nn.Sequential(*list(net.features.children())[14:23])\n",
    "        self.conv_block4 = nn.Sequential(*list(net.features.children())[24:33])\n",
    "        self.conv_block5 = nn.Sequential(*list(net.features.children())[34:43])\n",
    "        self.pool = nn.AvgPool2d(7, stride=1)\n",
    "        self.dpt = None\n",
    "        if dropout is not None:\n",
    "            self.dpt = nn.Dropout(dropout)\n",
    "        self.cls = nn.Linear(in_features=512+512+256, out_features=num_classes, bias=True)\n",
    "        \n",
    "       # initialize the attention blocks defined above\n",
    "        self.attn1 = AttentionBlock(256, 512, 256, 4, normalize_attn=normalize_attn)\n",
    "        self.attn2 = AttentionBlock(512, 512, 256, 2, normalize_attn=normalize_attn)\n",
    "        \n",
    "       \n",
    "        self.reset_parameters(self.cls)\n",
    "        self.reset_parameters(self.attn1)\n",
    "        self.reset_parameters(self.attn2)\n",
    "    def reset_parameters(self, module):\n",
    "        for m in module.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1.)\n",
    "                nn.init.constant_(m.bias, 0.)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0., 0.01)\n",
    "                nn.init.constant_(m.bias, 0.)\n",
    "    def forward(self, x):\n",
    "        block1 = self.conv_block1(x)       # /1\n",
    "        pool1 = F.max_pool2d(block1, 2, 2) # /2\n",
    "        block2 = self.conv_block2(pool1)   # /2\n",
    "        pool2 = F.max_pool2d(block2, 2, 2) # /4\n",
    "        block3 = self.conv_block3(pool2)   # /4\n",
    "        pool3 = F.max_pool2d(block3, 2, 2) # /8\n",
    "        block4 = self.conv_block4(pool3)   # /8\n",
    "        pool4 = F.max_pool2d(block4, 2, 2) # /16\n",
    "        block5 = self.conv_block5(pool4)   # /16\n",
    "        pool5 = F.max_pool2d(block5, 2, 2) # /32\n",
    "        N, __, __, __ = pool5.size()\n",
    "        \n",
    "        g = self.pool(pool5).view(N,512)\n",
    "        a1, g1 = self.attn1(pool3, pool5)\n",
    "        a2, g2 = self.attn2(pool4, pool5)\n",
    "        g_hat = torch.cat((g,g1,g2), dim=1) # batch_size x C\n",
    "        if self.dpt is not None:\n",
    "            g_hat = self.dpt(g_hat)\n",
    "        out = self.cls(g_hat)\n",
    "\n",
    "        return [out, a1, a2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7669d531-ef6f-4807-9ede-20e9309ae0cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T22:07:09.169110Z",
     "iopub.status.busy": "2022-06-15T22:07:09.168904Z",
     "iopub.status.idle": "2022-06-15T22:07:10.957471Z",
     "shell.execute_reply": "2022-06-15T22:07:10.956757Z",
     "shell.execute_reply.started": "2022-06-15T22:07:09.169088Z"
    }
   },
   "outputs": [],
   "source": [
    "model = AttnVGG(num_classes=1, normalize_attn=True)\n",
    "model=model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e12f7003-3fba-4062-809d-1ec8273c1a98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T22:11:14.028844Z",
     "iopub.status.busy": "2022-06-15T22:11:14.028596Z",
     "iopub.status.idle": "2022-06-15T22:11:14.035658Z",
     "shell.execute_reply": "2022-06-15T22:11:14.035053Z",
     "shell.execute_reply.started": "2022-06-15T22:11:14.028819Z"
    }
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, logits=False, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = targets.unsqueeze(1)\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "        \n",
    "criterion = FocalLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575c184e-c466-4028-b0c2-cb62add1e3f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9570705-4a82-403b-a8f2-a30faf45428f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T22:11:15.174717Z",
     "iopub.status.busy": "2022-06-15T22:11:15.174473Z",
     "iopub.status.idle": "2022-06-15T22:11:15.177814Z",
     "shell.execute_reply": "2022-06-15T22:11:15.177282Z",
     "shell.execute_reply.started": "2022-06-15T22:11:15.174691Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a4f66c-c428-47d9-b7bf-2e3e7ad38ae9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T22:11:15.536004Z",
     "iopub.status.busy": "2022-06-15T22:11:15.535815Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 66/92 [05:37<02:10,  5.01s/it]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "# import cv2 as cv\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "train_losses = []\n",
    "train_auc=[]\n",
    "val_auc=[]\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    train_preds=[]\n",
    "    train_targets=[]\n",
    "    auc_train=[]\n",
    "    loss_epoch_train=[]\n",
    "    # Run the training batches\n",
    "    for b, (X_train, y_train) in tqdm(enumerate(train_loader),total=len(train_loader)):\n",
    "        \n",
    "        b+=1\n",
    "        y_pred,_,_=model(X_train)\n",
    "        loss = criterion(torch.sigmoid(y_pred.type(torch.FloatTensor)), y_train.type(torch.FloatTensor))   \n",
    "        loss_epoch_train.append(loss.item())\n",
    "        # For plotting purpose\n",
    "        if (i==1):\n",
    "            if (b==19):\n",
    "                I_train = utils.make_grid(X_train[0:8,:,:,:], nrow=8, normalize=True, scale_each=True)\n",
    "                __, a1, a2 = model(X_train[0:8,:,:,:])\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "                 \n",
    "    try:\n",
    "        auc_train=roc_auc_score(y_train.detach().to(device).numpy(),torch.sigmoid(y_pred).detach().to(device).numpy())\n",
    "    except:\n",
    "        auc_train=0\n",
    "    train_losses.append(np.mean(loss_epoch_train))\n",
    "    train_auc.append(auc_train)\n",
    "    print(f'epoch: {i:2}   loss: {np.mean(loss_epoch_train):10.8f} AUC  : {auc_train:10.8f} ')\n",
    "    # Run the testing batches\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for b, (X_test, y_test) in enumerate(val_loader):\n",
    "            \n",
    "            y_val,_,_ = model(X_test)\n",
    "            loss = criterion(torch.sigmoid(y_val.type(torch.FloatTensor)), y_test.type(torch.FloatTensor))\n",
    "            loss_epoch_test.append(loss.item())\n",
    "    val_auc.append(auc_val)\n",
    "    print(f'Epoch: {i} Val Loss: {np.mean(loss_epoch_test):10.8f} AUC: {auc_val:10.8f} ')\n",
    "    \n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c46c6-5346-4153-86d7-97adc54e26fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(I_train,a,up_factor,no_attention=False):\n",
    "    img = I_train.permute((1,2,0)).cpu().numpy()\n",
    "    # compute the heatmap\n",
    "    if up_factor > 1:\n",
    "        a = F.interpolate(a, scale_factor=up_factor, mode='bilinear', align_corners=False)\n",
    "    attn = utils.make_grid(a, nrow=8, normalize=True, scale_each=True)\n",
    "    attn = attn.permute((1,2,0)).mul(255).byte().cpu().numpy()\n",
    "    attn = cv2.applyColorMap(attn, cv2.COLORMAP_JET)\n",
    "    attn = cv2.cvtColor(attn, cv2.COLOR_BGR2RGB)\n",
    "    attn = np.float32(attn) / 255\n",
    "    # add the heatmap to the image\n",
    "    img=cv2.resize(img,(466,60))\n",
    "    if no_attention:\n",
    "        return torch.from_numpy(img)\n",
    "    else:\n",
    "        vis = 0.6 * img + 0.4 * attn\n",
    "        return torch.from_numpy(vis)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b940c86-b0c4-447e-ac2b-0132bc061e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig=visualize_attention(I_train,a1,up_factor=2,no_attention=True)\n",
    "first=visualize_attention(I_train,a1,up_factor=2,no_attention=False)\n",
    "second=visualize_attention(I_train,a2,up_factor=4,no_attention=False)\n",
    "\n",
    "fig, (ax1, ax2,ax3) = plt.subplots(3, 1,figsize=(10, 10))\n",
    "ax1.imshow(orig)\n",
    "ax2.imshow(first)\n",
    "ax3.imshow(second)\n",
    "ax1.title.set_text('Input Images')\n",
    "ax2.title.set_text('pool-3 attention')\n",
    "ax3.title.set_text('pool-4 attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2721fb61-7185-4315-961a-027d6bdd9cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09509e00-7eea-4189-86bf-89fcabb78f42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd377448-e854-49c5-a3f1-88d9cced9839",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
